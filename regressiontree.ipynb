{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numdifftools as nd\n",
    "import math\n",
    "import numpy as np\n",
    "import bisect\n",
    "import pandas as pd\n",
    "import collections\n",
    "from math import log\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1 gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-16.087776514949773\n",
      "-27.467055951442692\n",
      "-34.69816537692322\n"
     ]
    }
   ],
   "source": [
    "def function(x):\n",
    "    return math.pow(x[0],4) + 4*x[0]*x[1] + 2 * x[1] + (x[1]**2)/2\n",
    "\n",
    "def grad(x,func):\n",
    "    return nd.Gradient(func)(x)\n",
    "\n",
    "\n",
    "def gradient_descentA(f, grad_f, eta, x_0, max_iter=100):\n",
    "    for i in range(max_iter):\n",
    "        x_t = x_0 - eta(i)* grad_f(x_0,f)\n",
    "        x_0 = x_t\n",
    "    return x_t\n",
    "\n",
    "def gradient_descentB(f, grad_f, eta, x_0, max_iter=100):\n",
    "    for i in range(max_iter):\n",
    "        x_t = x_0 - eta(i)* grad_f(x_0,f)\n",
    "        x_0 = x_t\n",
    "    return x_t\n",
    "\n",
    "def gradient_descentC(f,grad_f, eta,c,eta_start, x_0, milestones,max_iter=100):\n",
    "    for i in range(max_iter):\n",
    "        x_t = x_0 - eta(i,milestones,c,eta_start)* grad_f(x_0,f)\n",
    "        x_0 = x_t\n",
    "    return x_t\n",
    "\n",
    "def eta_const(t,c = 0.01):\n",
    "    return c \n",
    "\n",
    "def eta_sqrt(t, c = 0.1 ):\n",
    "    return c/np.sqrt(t+1)\n",
    "\n",
    "def eta_multistep(t, milestones, c, eta_init = 1):\n",
    "    interval = len(milestones) + 1\n",
    "    query_index = bisect.bisect_left(milestones, t)\n",
    "    res = [eta_init*math.pow(c,i) for i in range(interval)]\n",
    "    return res[query_index]\n",
    "\n",
    "\n",
    "\n",
    "x_0 = [1,1]\n",
    "A = gradient_descentA(f=function,grad_f=grad,eta=eta_const,x_0=x_0)\n",
    "B = gradient_descentB(f=function,grad_f=grad,eta=eta_sqrt,x_0=x_0)\n",
    "C = gradient_descentC(f=function,grad_f=grad,c=0.5,milestones=[10,60,90],eta_start=0.1,eta=eta_multistep ,x_0=x_0)\n",
    "\n",
    "valueA = function(A)\n",
    "valueB = function(B)\n",
    "valueC = function(C)\n",
    "\n",
    "\n",
    "\n",
    "print(valueA)\n",
    "print(valueB)\n",
    "print(valueC)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2 coordinate descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5773502691896257+2.2237664939066243e-30j)\n"
     ]
    }
   ],
   "source": [
    "x=[5,10,5]\n",
    "\n",
    "def argmin_xi(x):\n",
    "    x[0] = (0.5*x[1])**(1/3)\n",
    "    x[1] = 0.5*(x[0] - x[2])\n",
    "    x[2] = -0.5*x[1]\n",
    "    result = [x[0],x[1],x[2]]\n",
    "    return result\n",
    "\n",
    "def coordinate_descent(argmin,x_0,max_iter):\n",
    "    for i in range(max_iter):\n",
    "        x_1 = argmin(x_0)\n",
    "        x_0 = x_1\n",
    "    return x_1\n",
    "\n",
    "\n",
    "\n",
    "x1,x2,x3 = coordinate_descent(argmin=argmin_xi,x_0=x,max_iter=100)\n",
    "print(x1)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3 confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n",
      "0\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "def function(x):\n",
    "    return -0.078*x[0] - 0.227*x[1] + 0.165\n",
    "\n",
    "def confusion(l1,l2):  # l2: predicted, l1: ground truth\n",
    "    l1 = np.array(l1)\n",
    "    l2 = np.array(l2)\n",
    "    TP = sum(((l2 == 1) & (l1 == 1)) == 1).item()\n",
    "    FN = sum(((l2 == 0) & (l1 == 1)) == 1).item()\n",
    "    FP = sum(((l2 == 1) & (l1 == 0)) == 1).item()\n",
    "    TN = sum(((l2 == 0) & (l1 == 0)) == 1).item()\n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "\n",
    "x1 = [3,-2,2,-1.5,1.5,-2.5,-2,2]\n",
    "x2 = [1,2,2,2,3.5,2.5,0,2.5]\n",
    "y = [0,1,0,1,0,1,1,0]\n",
    "X = np.stack((x1, x2), axis=1)\n",
    "\n",
    "val = [function(X[i]) for i in range(len(X))]\n",
    "pred = np.array([bisect.bisect_left([0], i) for i in val])\n",
    "TP,TN,FP,FN = confusion(l1=y,l2=pred)\n",
    "print(TP)\n",
    "print(TN)\n",
    "print(FP)\n",
    "print(FN)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4 naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient diagnosis is: No disease.\n",
      "53.81% of the people in the training data has heart disease.\n",
      "The probability of having a normal thallium heart scan given that the patient has heart disease equals to 0.075.\n",
      "The accuracy of the model is 0.747.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "class naive_bayes_clf:\n",
    "    def __init__(self):\n",
    "        self.p_y = None\n",
    "        self.cpt_0 = None\n",
    "        self.cpt_1 = None\n",
    "\n",
    "    @staticmethod\n",
    "    def class_probability(data: pd.DataFrame) -> float:\n",
    "        \"\"\"\n",
    "        p(y) calculation\n",
    "        \"\"\"\n",
    "        return data[\"target\"].sum() / data.shape[0]\n",
    "\n",
    "    def conditional_probability(self, data: pd.DataFrame, feature: str, feature_value: int):\n",
    "        \"\"\"\n",
    "        p(x|y) calculation given conditional independence\n",
    "        \"\"\"\n",
    "        cf_0, cf_1 = data[data[\"target\"] == 0], data[data[\"target\"] == 1]\n",
    "\n",
    "        # Calculate probability of a feature having a value, given target 0 or 1, respectively.\n",
    "        prob_0 = cf_0[cf_0[feature] == feature_value][[feature]].count() / cf_0[[feature]].count()\n",
    "        prob_1 = cf_1[cf_1[feature] == feature_value][[feature]].count() / cf_1[[feature]].count()\n",
    "        f_index = list(data.columns).index(feature)\n",
    "        if f_index == 2:\n",
    "            feature_value -= 1\n",
    "\n",
    "        # Save conditional probability of feature value to the right indices\n",
    "        self.cpt_0[f_index][feature_value], self.cpt_1[f_index][feature_value] = prob_0[0], prob_1[0]\n",
    "\n",
    "    def train(self, data: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Train naive_bayes clf by calculating p(y) and p(x|y)\n",
    "        \"\"\"\n",
    "        self.p_y = self.class_probability(data)\n",
    "        self.cpt_0 = [[0 for _ in data[col].unique()] for col in data.columns[:-1]]\n",
    "        self.cpt_1 = [[0 for _ in data[col].unique()] for col in data.columns[:-1]]\n",
    "        for col in data.columns[:-1]:\n",
    "            for val in data[col].unique():\n",
    "                self.conditional_probability(data, col, val)\n",
    "\n",
    "        return self.p_y, self.cpt_0, self.cpt_1\n",
    "\n",
    "    def predict(self, vector: list) -> int:\n",
    "        \"\"\"\n",
    "        Predict outcome of vector by p(y) * âˆ p(x|y)\n",
    "        \"\"\"\n",
    "        assert all([val is not None for val in [self.p_y, self.cpt_0, self.cpt_1]]), \"First train the model.\"\n",
    "        prob_0, prob_1 = 1, 1\n",
    "        vector[2] -= 1\n",
    "\n",
    "        # p(x|y) multiplication for conditional independence\n",
    "        for feature, value in zip(self.cpt_0, vector):\n",
    "            prob_0 *= feature[value]\n",
    "\n",
    "        for feature, value in zip(self.cpt_1, vector):\n",
    "            prob_1 *= feature[value]\n",
    "\n",
    "        # multiply by p(y) to get p(y|x)\n",
    "        prob_0 *= 1 - self.p_y\n",
    "        prob_1 *= self.p_y\n",
    "\n",
    "        return 0 if prob_0 > prob_1 else 1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_data = pd.read_csv(r'heart_train_data.csv')\n",
    "    validation_data = pd.read_csv(r'heart_validate_data.csv')\n",
    "\n",
    "    model = naive_bayes_clf()\n",
    "    class_prob, cpt_0, cpt_1 = model.train(train_data)\n",
    "\n",
    "    # 5A\n",
    "    result = model.predict(vector=[1, 1, 2])\n",
    "    print(f\"Patient diagnosis is: {'No disease' if result == 1 else 'Disease'}.\")\n",
    "\n",
    "    # 5B\n",
    "    atypical_angina = train_data.cp.value_counts()[1] / sum(train_data.cp.value_counts())\n",
    "    ex_ang = train_data.exang.value_counts()[1] / sum(train_data.exang.value_counts())\n",
    "    tha_l = train_data.thal.value_counts()[1] / sum(train_data.thal.value_counts())\n",
    "    abcde = atypical_angina+ex_ang+tha_l\n",
    "    print(f\"{round(abcde*100, 3)}% of the people in the training data has heart disease.\")\n",
    "\n",
    "    # 5C\n",
    "    print(f\"The probability of having a normal thallium heart scan given that the patient has heart disease \"\n",
    "          f\"equals to {round(cpt_0[2][0], 3)}.\")\n",
    "\n",
    "    # 5D\n",
    "    x_validate = validation_data.iloc[:, :-1]\n",
    "    y_true = list(validation_data.iloc[:, -1])\n",
    "    y_pred = [model.predict(vector=row[1]) for row in x_validate.iterrows()]\n",
    "\n",
    "    accuracy = sum(1 for x, y in zip(y_true, y_pred) if x == y) / len(y_true)\n",
    "\n",
    "    print(f\"The accuracy of the model is {round(accuracy, 3)}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5 decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_dataset = pd.read_csv(r'heart_train_data.csv')\n",
    "validate_dataset = pd.read_csv(r'heart_validate_data.csv')\n",
    "dataset = train_dataset.values.tolist()\n",
    "validate_dataset = validate_dataset.values.tolist()\n",
    "impurity = []\n",
    "labels = ['cp','exang','thal']\n",
    "targetLabels = ['a heart disease',' no heart disease']\n",
    "\n",
    "\n",
    "def calEntropy(dataset):\n",
    "    #Calculate entropy which is used as impurity measure\n",
    "    y_data = [i[-1] for i in dataset]\n",
    "    entropy = 0\n",
    "    count = collections.Counter(y_data)\n",
    "    #Formula entropy=sum(-log(p,2)*p)\n",
    "    for i in range(len(count)-1):\n",
    "        p = count[i]/len(y_data)\n",
    "        entropy -= p * log(p, 2)\n",
    "    return  entropy\n",
    "\n",
    "def splitDataset(dataset, feature, value):\n",
    "    #split the dataset set into two parts according to the selected features and values\n",
    "    subdataset1 = []\n",
    "    subdataset2 = []\n",
    "    # subdataset1 with the feature equals to value\n",
    "    # subdataset2  with the frature not equals to value\n",
    "    for featVec in dataset: \n",
    "        if featVec[feature] == value:\n",
    "            subdataset1.append(featVec)\n",
    "        else:\n",
    "            subdataset2.append(featVec)\n",
    "    return  subdataset1,subdataset2\n",
    "\n",
    "def bestFeature(dataset):\n",
    "    #Calculate the bestway to split the dataset by comparing the entropy.\n",
    "    numFeatures = len(dataset[0]) - 1\n",
    "    bestEntropy = 999\n",
    "    bestFeature = -1\n",
    "    bestValue = -1\n",
    "    # Traverse all the features and values to splite the data. \n",
    "    # And calculate the entropy of each way of spliting to find the lowest entropy.\n",
    "    for i in range(numFeatures):\n",
    "        featList = [example[i] for example in dataset]\n",
    "        uniqueVals = set(featList)\n",
    "        uniqueVals = list(uniqueVals)\n",
    "        entropy=0.0\n",
    "        for value in uniqueVals[0:len(uniqueVals)]:\n",
    "            subdataset1,subdataset2 = splitDataset(dataset, i, value)\n",
    "            p = len(subdataset1) / float(len(dataset))\n",
    "            entropy = p * calEntropy(subdataset1) + (1-p)*calEntropy(subdataset2)\n",
    "            if (entropy < bestEntropy):\n",
    "                bestEntropy = entropy\n",
    "                bestFeature = i\n",
    "                bestValue = value\n",
    "    return bestFeature,bestValue\n",
    "\n",
    "def calLeaf(dataset):\n",
    "    #determine the node value based on the majority of data\n",
    "    y_data = [i[-1] for i in dataset]\n",
    "    count = collections.Counter(y_data)\n",
    "    if count[0]>count[1]:\n",
    "        impurity.append(1-count[0]/len(y_data))\n",
    "        return 0\n",
    "    else:\n",
    "        impurity.append(1-count[1]/len(y_data))\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cartTree(object):\n",
    "    def __init__(self,dataset = None,depth = 0):\n",
    "        #Each node represents a choice. \n",
    "        #Feature and value is the way to choose.\n",
    "        #Left and right represents the data that has been splited by the choice. \n",
    "        #Depth is the layer of ndoe to determine the termination conditions.\n",
    "        self.dataset = dataset\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.feature = -2\n",
    "        self.value = -2\n",
    "        self.depth = depth\n",
    "    \n",
    "    def creatTree(self,dataset,depth):\n",
    "        #Creat the decision tree.\n",
    "        self.dataset = dataset\n",
    "        feature = -1\n",
    "        value = -1\n",
    "        self.depth = depth\n",
    "        if self.depth < 3:\n",
    "            feature,value = bestFeature(dataset)\n",
    "            self.feature = feature\n",
    "            self.value = value\n",
    "            subdataset1,subdataset2 = splitDataset(dataset, feature, value)\n",
    "            #Recurse on child nodes\n",
    "            self.left = cartTree(subdataset1,depth+1)\n",
    "            self.left.creatTree(subdataset1,depth+1)\n",
    "            self.right = cartTree(subdataset2,depth+1)\n",
    "            self.right.creatTree(subdataset2,depth+1)\n",
    "        else:\n",
    "            self.value = calLeaf(dataset)\n",
    "            self.feature = -1\n",
    "    \n",
    "    def calAccuracy(self,dataset):\n",
    "        #Count the number of correct predictions in the validate_dataset.\n",
    "        feature = self.feature\n",
    "        value = self.value\n",
    "        sumRight = 0\n",
    "        if feature != -1:\n",
    "            #If it is not a leaf node, calculate the sum of left and right.\n",
    "            subdataset1,subdataset2 = splitDataset(dataset, feature, value)\n",
    "            sum1 = self.left.calAccuracy(subdataset1)\n",
    "            sum2 = self.right.calAccuracy(subdataset2)\n",
    "            sumRight = sum1 + sum2\n",
    "            return sumRight\n",
    "        else:\n",
    "             #If it is a leaf node, return the number of correct predictions due to the value of leaf node.\n",
    "            y_data=[i[-1] for i in dataset]\n",
    "            count = collections.Counter(y_data)\n",
    "            if self.value == 0:\n",
    "                return count[0]\n",
    "            else:\n",
    "                return count[1]\n",
    "            \n",
    "    def calPatient(self,pdata):\n",
    "        #calculate the result of a certain patient\n",
    "        node = cartTree() \n",
    "        node = self\n",
    "        for i in pdata:\n",
    "            if i == self.value:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.725275 \n",
      "The decision of the root of the tree is based on cp , value = 0  \n",
      "This patient has  no heart disease\n",
      "The impurity of each leaf is\n",
      "    0.347826\n",
      "    0.200000\n",
      "    0.181818\n",
      "    0.048780\n",
      "    0.368421\n",
      "    0.333333\n",
      "    0.222222\n",
      "    0.064103\n"
     ]
    }
   ],
   "source": [
    "impurity = []\n",
    "tree = cartTree()\n",
    "tree.creatTree(dataset,0)\n",
    "# A\n",
    "right = tree.calAccuracy(validate_dataset)\n",
    "accuracy = right/len(validate_dataset)\n",
    "print('accuracy = %f ' %accuracy)\n",
    "# B\n",
    "rootFeature,rootValue = bestFeature(dataset)\n",
    "print('The decision of the root of the tree is based on %s , value = %d  ' %(labels[rootFeature], rootValue))\n",
    "# C\n",
    "patient = [1,1,2]\n",
    "result = tree.calPatient(patient)\n",
    "print('This patient has %s' %targetLabels[result])\n",
    "# D\n",
    "print('The impurity of each leaf is')\n",
    "for i in impurity:\n",
    "    print('    %f' %i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "447218537823dc48605bf3e3bcf0f395dabc5dc8aa93a3b4f8b9010990ed5cfe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
